{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Optimization using JuMP\n",
    "\n",
    "\n",
    "# JuMP\n",
    "\n",
    "[JuMP](https://github.com/JuliaOpt/JuMP.jl) is a Julia package that provides a modeling language for general optimization problems.\n",
    "\n",
    "## Optimization in Julia\n",
    "\n",
    "Julia's optimization packages have become popular due to their ease of use, and variety of interfaces to mature solvers.  The main optimization functionality in Julia is provided by [JuliaOpt](http://www.juliaopt.org/).\n",
    "\n",
    "The two high-level interfaces that you are most likely to use are\n",
    "* [JuMP](https://github.com/JuliaOpt/JuMP.jl) - a modeling language for all sorts of optimization problems\n",
    "* [Convex.jl](https://github.com/JuliaOpt/Convex.jl) - a package for disciplined convex programming (like [CVX](http://cvxr.com/cvx/))\n",
    "\n",
    "There is a mid-level interface as well, [MathProgBase.jl](https://github.com/JuliaOpt/MathProgBase.jl).\n",
    "\n",
    "The powerful thing about all of the above is that it is largely **solver independent**. This means you can forumlate the optimization problem with these packages, and then choose from a variety of solvers to use under the hood.  This is just like other modeling languages like AMPL - the reason why Julia's optimization packages have become popular is that they are generally easier to use than older modeling languages.\n",
    "\n",
    "## Solvers\n",
    "\n",
    "Today is more about turning your optimization models into something that can run on a computer via JuMP, but you still need a solver to actually solve the problem for you under the hood.  [JuliaOpt](http://www.juliaopt.org/) has a list of solvers that can be called from the high level interfaces (there are currently 20).  There are many open-source options available, but there are also interfaces to some of the big commercial solvers such as [Gurobi](http://www.gurobi.com/), [Mosek](https://www.mosek.com/), [Knitro](https://www.artelys.com/en/optimization-tools/knitro), etc. Many of these commercial solvers offer free academic/student licences, and if you are trying to solve large optimization problems they may be worth looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not already installed JuMP and a solver\n",
    "Pkg.add(\"JuMP\")\n",
    "Pkg.add(\"Clp\")   # solver\n",
    "Pkg.add(\"Ipopt\") # solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using Clp\n",
    "using Cbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Problems\n",
    "\n",
    "The hardest part of solving an optimization problem is often formulating the problem itself.  Optimization problems generally take the form\n",
    "\n",
    "$$\n",
    "\\text{minimize}_x~f(x)\\\\\n",
    "\\text{subject to}~c_e(x) = 0; c_i(x) \\ge 0\n",
    "$$\n",
    "\n",
    "where $f(x)$ is some function, $c_e(x)$ encodes equality constraints, and $c_i(x)$ encodes inequality constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Programming\n",
    "\n",
    "[Linear Programs (LPs)](https://en.wikipedia.org/wiki/Linear_programming) are optimization problems for which both the objective and constraint functions are linear.\n",
    "\n",
    "$$\n",
    "\\text{minimize}_{x\\in \\mathbb{R}^n}~c^T x\\\\\n",
    "\\text{subject to}~Ax = a; Bx \\ge b\n",
    "$$\n",
    "\n",
    "One place LPs arise are in maximizing profit.\n",
    "\n",
    "Here's a very simple LP we can easily solve by hand:\n",
    "\n",
    "$$\n",
    "\\text{minimize}~x+y\\\\\n",
    "\\text{subject to:}\\\\\n",
    "\\qquad x\\ge 0\\\\\n",
    "\\qquad y \\ge 0\\\\\n",
    "\\qquad x+y \\le 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Cbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "# m = Model(solver=ClpSolver())\n",
    "m = Model(solver=GurobiSolver())\n",
    "# add variables\n",
    "@variable(m, x >= 0)\n",
    "@variable(m, y >= 0)\n",
    "# add additional constraint\n",
    "@constraint(m, x+y <= 1)\n",
    "# define objective\n",
    "@objective(m, Min, x+y)\n",
    "# prints problem we've defined\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the problem\n",
    "solve(m)\n",
    "# print objective value, and the values of x,y\n",
    "println(\"Optimal objective: \",getobjectivevalue(m), \n",
    "\t\". x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Install another solver that will solve a Linear Program (see [JuliaOpt](http://www.juliaopt.org/) for options - look at the first column), and solve the above example using the new solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Programming\n",
    "\n",
    "[Quadratic Programs (QPs)](https://en.wikipedia.org/wiki/Quadratic_programming) allow the objective function $f(x)$ to be quadratic.  The optimization problems have the form\n",
    "\n",
    "$$\n",
    "\\text{minimize}_{x\\in \\mathbb{R}^n}~\\tfrac{1}{2}x^T Q x+c^T x\\\\\n",
    "\\text{subject to}~Ax \\le b\n",
    "$$\n",
    "\n",
    "Note that in the case that $Q$ is SPD, that the problem is convex, but this is generally not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Ipopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "m = Model(solver=IpoptSolver())\n",
    "# add variables\n",
    "@variable(m, x >= 0)\n",
    "@variable(m, y >= 0)\n",
    "# add additional constraint\n",
    "@constraint(m, x+y <= 1)\n",
    "# define objective\n",
    "@objective(m, Min, (x-0.5)^2 + y)\n",
    "# prints problem we've defined\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)\n",
    "println(\"Optimal objective: \",getobjectivevalue(m), \n",
    "\t\". x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-order Cone Constraints\n",
    "\n",
    "http://www.juliaopt.org/JuMP.jl/0.18/refmodel.html#second-order-cone-constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integer Programming\n",
    "\n",
    "[Integer Programs (IPs)](https://en.wikipedia.org/wiki/Integer_programming) add integer constraints to the optimization variables.  This makes sense when variables come in unit quantities (e.g. people)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Cbc\n",
    "m = Model(solver=CbcSolver())\n",
    "@variable(m, x >= 0, Int) # Int keyword says that x should be an integer\n",
    "@variable(m, y >= 0, Int)\n",
    "\n",
    "@objective(m, Max, x + y)\n",
    "@constraint(m, 50x + 24y <= 2400)\n",
    "@constraint(m, 30x + 33y <= 2100)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)\n",
    "println(\"Optimal objective: \",getobjectivevalue(m), \n",
    "\t\". x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Integer Programming\n",
    "\n",
    "Mixed Integer Programs (MIPs) have some variables that are constrained to be integers, and some that are not.\n",
    "\n",
    "callbacks:\n",
    "\n",
    "http://www.juliaopt.org/JuMP.jl/0.18/callbacks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Cbc\n",
    "m = Model(solver=CbcSolver())\n",
    "@variable(m, x >= 0, Int) # Int keyword says that x should be an integer\n",
    "@variable(m, y >= 0)\n",
    "\n",
    "@objective(m, Max, x + y)\n",
    "@constraint(m, 50x + 24y <= 2400)\n",
    "@constraint(m, 30x + 33y <= 2100)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)\n",
    "println(\"Optimal objective: \",getobjectivevalue(m), \n",
    "\t\". x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - NP-complete problems\n",
    "\n",
    "[NP-complete](https://en.wikipedia.org/wiki/NP-completeness) problems are decision problems that may have no polynomial-time algorithm to compute, although solutions can be verified in polynomial time.  They arise in computer science, operations research, and have a variety of applications.\n",
    "\n",
    "Many of these problems can be formulated as optimization problems with integer constraints.\n",
    "\n",
    "* Minimum Vertex Cover https://en.wikipedia.org/wiki/Vertex_cover\n",
    "* Maximum Coverage https://en.wikipedia.org/wiki/Maximum_coverage_problem\n",
    "* Max-Cut (Need commercial solver for QIP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum-Coverage\n",
    "\n",
    "The Maximum-Coverage problem is as follows: given $m$ sets, and an integer $k$, maximize the number of elements covered by at most $k$ sets. We'll assume that each of the $m$ sets is a subset of $N$ elements.\n",
    "\n",
    "An application to think of: you want to buy $N$ items.  There are $m$ stores you could possibly go to, and each store only carries some of the items you'd like to buy.  Unfortunately you only have time to go to $k$ of the stores, but you'd like to get as much of your shopping done as possible (maximize the number of items you get).  Which stores should you go to today?\n",
    "\n",
    "### Step 1: Formulate an Integer Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a Maximum-Coverage problem\n",
    "N = 15\n",
    "nS = 6\n",
    "k = 2\n",
    "x = collect(1:N) # set of items\n",
    "# create assign elements to the nS sets\n",
    "S = Vector{Vector{Int64}}(N)\n",
    "for i = 1:N\n",
    "   S[i] = unique([rand(1:nS) rand(1:nS)])\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Cbc\n",
    "m = Model(solver=CbcSolver())\n",
    "\n",
    "@variable(m, 0 <= y[1:N] <= 1, Int) # denotes if element in chosen set\n",
    "@variable(m, 0 <= x[1:nS] <= 1, Int) # denotes if set is chosen\n",
    "@objective(m, Max, sum(y))\n",
    "\n",
    "@constraint(m, sum(x) <= k) # choose at most k sets\n",
    "\n",
    "# if y[i] is chosen, then it must be in at least 1 chosen set\n",
    "for i = 1:N\n",
    "    @constraint(m, sum(x[S[i]]) >= y[i])\n",
    "end\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solve(m)\n",
    "@show getvalue(x)\n",
    "sum(getvalue(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random assignment\n",
    "# choose nS random sets\n",
    "x_rand = zeros(nS)\n",
    "for i = 1:k\n",
    "    found = false\n",
    "   while !found\n",
    "       i = rand(1:nS)\n",
    "        if x_rand[i] == 0\n",
    "            x_rand[i] = 1\n",
    "            found = true\n",
    "        end\n",
    "    end\n",
    "end\n",
    "@show x_rand\n",
    "y_rand = zeros(N)\n",
    "for i = 1:N\n",
    "   if sum(x_rand[S[i]]) > 0\n",
    "        y_rand[i] = 1\n",
    "    end\n",
    "end\n",
    "sum(y_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: LP relaxation\n",
    "\n",
    "We can turn the integer program to a linear program by relaxing the constraint that the variables need to be integers.  After solving the LP, we then round the solution to integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Clp\n",
    "m = Model(solver=ClpSolver())\n",
    "\n",
    "@variable(m, 0 <= y[1:N] <= 1) # denotes if element in chosen set\n",
    "@variable(m, 1>= x[1:nS] >= 0) # denotes if set is chosen\n",
    "@objective(m, Max, sum(y))\n",
    "\n",
    "@constraint(m, sum(x) <= k) # choose at most k sets\n",
    "\n",
    "# if y[i] is chosen, then it must be in at least 1 chosen set\n",
    "for i = 1:N\n",
    "    @constraint(m, sum(x[S[i]]) >= y[i])\n",
    "end\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)\n",
    "@show getvalue(x)\n",
    "sum(getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Max-Cut of a Graph\n",
    "\n",
    "The [Max-Cut](https://en.wikipedia.org/wiki/Maximum_cut) problem is to partition the vertices $V$ of a weighted graph $G = (V,E,W)$ into two sets $S,\\bar{S}$ that maximizes the weight of the cut edges.\n",
    "$$ \n",
    "\\text{maximize}_S ~\\sum_{i\\in S} \\sum_{j\\in\\bar{S}} w_{ij}\n",
    "$$\n",
    "We see that this is equivalent to the integer QP (http://www-math.mit.edu/~goemans/PAPERS/maxcut-jacm.pdf)\n",
    "$$\n",
    "\\text{maximize} ~\\tfrac{1}{2} \\sum_{i<j} w_{ij} (1-y_i y_j)\\\\\n",
    "\\text{subject to:} ~ y_i\\in\\{-1,1\\}\n",
    "$$\n",
    "We see that maximizing the above objective is equivalent to maximizing\n",
    "$$\n",
    "-\\sum_{i<j} w_{ij} y_i y_j = y^T W y\n",
    "$$\n",
    "Where $W$ is the weighted adjacency matrix of $G$\n",
    "\n",
    "\n",
    "To simplify everything, we'll assume all weights are 1, so that $W$ is just the adjacency matrix of the graph.  We'll also work on a graph that we know the answer to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line graph\n",
    "A = 1.0*[0 1 0; 1 0 1; 0 1 0]\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function obj_value(y, W)\n",
    "   return 0.25*(sum(W) -y'*W*y)\n",
    "end\n",
    "\n",
    "# if weights are 0, randomly assign\n",
    "function round_weights{T}(y::Vector{T})\n",
    "    n = length(y)\n",
    "    z = Array{T}(n)\n",
    "    @simd for i = 1:n\n",
    "        z[i] = (y[i] == T(0)) ? sign(rand()) : sign(y[i])\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "\n",
    "# random weights\n",
    "y = round_weights(randn(3))\n",
    "obj_value(y, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Relaxation\n",
    "\n",
    "We can relax the above problem to:\n",
    "$$\n",
    "\\text{maximize}_y ~-y^T W y\\\\\n",
    "\\text{subject to:} ~-1 \\le y_i \\le y\n",
    "$$\n",
    "And round the solution to $1$ or $-1$\n",
    "\n",
    "1. What sort of optimization problem is the relaxation? (LP, QP, IP, MIP, ...)  What's an appropriate solver to use?\n",
    "2. Use JuMP to model the above problem, find the solution, and round the vertex weights.\n",
    "3. Try both graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Ipopt\n",
    "m = Model(solver=IpoptSolver())\n",
    "@variable(m, -1 <= y[1:3] <= 1) # Int keyword says that x should be an integer\n",
    "@objective(m, Max, -y'*A*y)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getvalue(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Full Integer Program\n",
    "\n",
    "1. use JuMP to model the original integer program. What is an appropriate solver to use?\n",
    "2. Which option is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SCIP\n",
    "# m = Model(solver=AmplNLSolver(\"couenne\"))\n",
    "m = Model(solver=SCIPSolver())\n",
    "@variable(m, -1 <= y[1:3] <= 1, Int) \n",
    "@objective(m, Max, -y'*A*y)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)\n",
    "yrnd = round_weights(getvalue(y))\n",
    "obj_value(yrnd, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Programming\n",
    "\n",
    "JuMP also allows you to model more general [nonlinear problems (NLPs)](https://en.wikipedia.org/wiki/Nonlinear_programming).  To do this, you will want to use the macros `@NLobjective`, and `@NLconstraint`.  These can be combined with objectives and constraints we've already seen.\n",
    "\n",
    "* JuMP's Introduction to solving NLPs [here](http://www.juliaopt.org/JuMP.jl/0.18/nlp.html)\n",
    "\n",
    "## Example\n",
    "\n",
    "Here, we'll look at the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function)\n",
    "$$\n",
    "f(x,y) = (a-x)^2 + b(y-x^2)^2\n",
    "$$\n",
    "For $b>0$, this function has a global minimum at $(x,y) = (a,a^2)$, where $f(x,y) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plotlyjs()\n",
    "a = 1.0\n",
    "b = 50.0\n",
    "f(x,y) = (a - x)^2 + b*(y - x^2)^2\n",
    "n = 100\n",
    "xs = ones(n)*linspace(-1.5,1.5,n)'\n",
    "ys = linspace(-1.5,1.5,n)*ones(n)'\n",
    "fs = f.(xs, ys)\n",
    "surface(xs, ys, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From http://www.juliaopt.org/JuMP.jl/0.18/nlp.html\n",
    "using JuMP, Ipopt\n",
    "m = Model(solver=IpoptSolver())\n",
    "@variable(m, x, start = 0.0)\n",
    "@variable(m, y, start = 0.0)\n",
    "\n",
    "@NLobjective(m, Min, (1-x)^2 + 100(y-x^2)^2)\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Most optimization solvers need gradients and hessians in order to work.  How does JuMP obtain this information?\n",
    "\n",
    "The answer is that all this information is obtained through [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) which uses the rules of differential calculus to differentiate functions just like you would.  Note that **this is different from using finite difference schemes** and is typically much more accurate.\n",
    "\n",
    "The packages that JuMP uses for automatic differentiation are\n",
    "* [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)\n",
    "* [Calculus.jl](https://github.com/JuliaMath/Calculus.jl)\n",
    "\n",
    "You can also use these packages for your own purposes outside of JuMP.  As long as a function is built from core functions (e.g. you aren't calling special function libraries), products and sums, you can differentiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "f(x::Vector) = sum(x)\n",
    "n = 5\n",
    "x = randn(5)\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.gradient(f,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.hessian(f,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also wrap the gradient and hessian functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x) = ForwardDiff.gradient(f,x)\n",
    "g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example on a more complicated function\n",
    "f(x::Vector) = x[1]*x[2] + sum(sin.(x[3:end]))\n",
    "g(x) = ForwardDiff.gradient(f,x)\n",
    "g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Custom Functions for NLP in JuMP\n",
    "\n",
    "Above, we wrote the Rosenbrock function explicitly as the objective function\n",
    "```julia\n",
    "@NLobjective(m, Min, (1-x)^2 + 100(y-x^2)^2)\n",
    "```\n",
    "We can also provide a function wrapper for the function - all we need to do is \"register\" the function in JuMP.  You can find informaiton in [JuMP's documentation here](http://www.juliaopt.org/JuMP.jl/0.18/nlp.html#user-defined-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt\n",
    "\n",
    "rosenbrock(x,y) = (a - x)^2 + b*(y - x^2)^2\n",
    "\n",
    "m = Model(solver=IpoptSolver(print_level=0))\n",
    "\n",
    "# registers function with JuMP, and derivatives are computed\n",
    "JuMP.register(m, :rosenbrock, 2, rosenbrock, autodiff=true)\n",
    "\n",
    "@variable(m, x, start = 0.0)\n",
    "@variable(m, y, start = 0.0)\n",
    "\n",
    "@NLobjective(m, Min, rosenbrock(x,y))\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the register command:\n",
    "```julia\n",
    "JuMP.register(m, :rosenbrock, 2, rosenbrock, autodiff=true)\n",
    "```\n",
    "| argument | description |\n",
    "| ------ | -------- |\n",
    "| `m` | model |\n",
    "| `:rosenbrock` | Symbol used to identify funciton in objective|\n",
    "| `2` | number of scalar inputs |\n",
    "| `rosenbrock` | function we've declared |\n",
    "\n",
    "If you can't forward diff your package, or you have an optimized gradient, you can also pass those in explicitly.  Note that JuMP currently doesn't support Hessians for functions of more than one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x,y) = x^2 + y^2\n",
    "function ∇f(g,x,y) \n",
    "    g[1] = 2*x\n",
    "    g[2] = 2*y\n",
    "end\n",
    "\n",
    "using JuMP, Ipopt\n",
    "\n",
    "m = Model(solver=IpoptSolver())\n",
    "\n",
    "# registers function with JuMP, and derivatives are computed\n",
    "JuMP.register(m, :f, 2, f, ∇f)\n",
    "\n",
    "@variable(m, x, start = 1.0)\n",
    "@variable(m, y, start = 1.0)\n",
    "\n",
    "@NLobjective(m, Min, f(x,y))\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Constraints\n",
    "\n",
    "You can specify noninear constraints using the `@NLconstraint` macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt\n",
    "\n",
    "m = Model(solver=IpoptSolver())\n",
    "\n",
    "@variable(m, x, start = 1.0)\n",
    "@variable(m, y, start = 1.0)\n",
    "\n",
    "@NLobjective(m, Min, (x-2)^4 + y^2)\n",
    "\n",
    "@NLconstraint(m, x^2 - y == 0)\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Finding the largest eigenvalue of a symmetric matrix can be formulated as an optimization problem\n",
    "\n",
    "$$\n",
    "\\text{maximize}~x^T A x\\\\\n",
    "\\text{subject to:}~ \\|x\\|_2^2 = 1\n",
    "$$\n",
    "\n",
    "The constraint $\\|x\\|_2^2 = x^Tx = 1$ is non-linear (and non-convex).  Find the largest eigenpair of `A = Diagonal([2; 1])` using JuMP.\n",
    "\n",
    "Try this on a larger (symmetric) matrix.  How long does this take compared to `eig`, or `eigs`?\n",
    "\n",
    "To time an operation, use Julia's `@time` macro, e.g. `@time solve(m)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt\n",
    "\n",
    "m = Model(solver=IpoptSolver(print_level=0))\n",
    "\n",
    "A = Diagonal([2;1])\n",
    "\n",
    "@variable(m, x[1:2], start=randn())\n",
    "\n",
    "@objective(m, Max, x'*A*x)\n",
    "\n",
    "@NLconstraint(m, x[1]^2 + x[2]^2 == 1)\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Find a local optimum for the folowing optimization problem\n",
    "\n",
    "$$\n",
    "\\text{minimize}~x^3 + y^2 - x^4 + y + z\\\\\n",
    "\\text{subject to:}~x^2 + y^2 + z^2 \\le 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras\n",
    "\n",
    "We used the Ipopt solver in our demo NLPs.  Check out [NLopt.jl](https://github.com/JuliaOpt/NLopt.jl) for another (free/open) option.  This is an itnterface to the [NLopt](https://nlopt.readthedocs.io/en/latest/) library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
