{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Optimization using JuMP\n",
    "\n",
    "\n",
    "# JuMP\n",
    "\n",
    "[JuMP](https://github.com/JuliaOpt/JuMP.jl) is a Julia package that provides a modeling language for general optimization problems.\n",
    "\n",
    "## Optimization in Julia\n",
    "\n",
    "Julia's optimization packages have become popular due to their ease of use, and variety of interfaces to mature solvers.  The main optimization functionality in Julia is provided by [JuliaOpt](http://www.juliaopt.org/).\n",
    "\n",
    "The two high-level interfaces that you are most likely to use are\n",
    "* [JuMP](https://github.com/JuliaOpt/JuMP.jl) - a modeling language for all sorts of optimization problems\n",
    "* [Convex.jl](https://github.com/JuliaOpt/Convex.jl) - a package for disciplined convex programming (like [CVX](http://cvxr.com/cvx/))\n",
    "\n",
    "There is a mid-level interface as well, [MathProgBase.jl](https://github.com/JuliaOpt/MathProgBase.jl).\n",
    "\n",
    "The powerful thing about all of the above is that it is largely **solver independent**. This means you can forumlate the optimization problem with these packages, and then choose from a variety of solvers to use under the hood.  This is just like other modeling languages like AMPL - the reason why Julia's optimization packages have become popular is that they are generally easier to use than older modeling languages.\n",
    "\n",
    "## Solvers\n",
    "\n",
    "Today is more about turning your optimization models into something that can run on a computer via JuMP, but you still need a solver to actually solve the problem for you under the hood.  [JuliaOpt](http://www.juliaopt.org/) has a list of solvers that can be called from the high level interfaces (there are currently 20).  There are many open-source options available, but there are also interfaces to some of the big commercial solvers such as [Gurobi](http://www.gurobi.com/), [Mosek](https://www.mosek.com/), [Knitro](https://www.artelys.com/en/optimization-tools/knitro), etc. Many of these commercial solvers offer free academic/student licences, and if you are trying to solve large optimization problems they may be worth looking at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Class\n",
    "\n",
    "Last class, we saw JuMP for the first time.  We covered linear programs (LPs), quadratic programs (QPs), second order cone constraints, and semi-definite programs (SDPs).  All of these problems arise have important applications in science and engineering.\n",
    "\n",
    "# Today\n",
    "\n",
    "We're going to cover several other types of optimization problems:\n",
    "* Integer Programs (IPs)\n",
    "* Mixed Integer Programs (MIPs)\n",
    "* Nonlinear Programs (NLPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integer Programming\n",
    "\n",
    "[Integer Programs (IPs)](https://en.wikipedia.org/wiki/Integer_programming) add integer constraints to the optimization variables.  This makes sense when variables come in unit quantities (e.g. people).  These problems take the form\n",
    "\\begin{align*}\n",
    "\\underset{x}{\\text{minimize}} &~f(x)\\\\\n",
    "\\text{subject to:} &~x\\in \\mathbb{Z}\\\\\n",
    "&c_e(x) = 0\\\\\n",
    "&c_i(x) \\le 0\\\\\n",
    "\\end{align*}\n",
    "In JuMP, you can specify if a variable is an integer inside the variable declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Cbc\n",
    "m = Model(solver=CbcSolver())\n",
    "@variable(m, x >= 0, Int) # Int keyword says that x should be an integer\n",
    "@variable(m, y >= 0, Int)\n",
    "\n",
    "@objective(m, Max, x + y)\n",
    "@constraint(m, 2*x + 2*y <= 5)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)\n",
    "println(\"Optimal objective: \",getobjectivevalue(m), \n",
    "\t\". x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is also a `Bin` keyword that constrains a variable to be in {0,1}.\n",
    "\n",
    "This is good for variables that denote if something exists or doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Cbc\n",
    "m = Model(solver=CbcSolver())\n",
    "@variable(m, x >= 0, Bin) # Int keyword says that x should be an integer\n",
    "@variable(m, y >= 0, Int)\n",
    "\n",
    "@objective(m, Max, 2*x + y)\n",
    "@constraint(m, 2*x + 2*y <= 7)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)\n",
    "println(\"Optimal objective: \",getobjectivevalue(m), \n",
    "\t\". x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Integer Programming\n",
    "\n",
    "Mixed Integer Programs (MIPs) have some variables that are constrained to be integers, and some that are not.\n",
    "\n",
    "callbacks:\n",
    "\n",
    "http://www.juliaopt.org/JuMP.jl/0.18/callbacks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Cbc\n",
    "m = Model(solver=CbcSolver())\n",
    "@variable(m, x >= 0, Int) # Int keyword says that x should be an integer\n",
    "@variable(m, y >= 0)\n",
    "\n",
    "@objective(m, Max, x + y)\n",
    "@constraint(m, 2*x + 2*y <= 5)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)\n",
    "println(\"Optimal objective: \",getobjectivevalue(m), \n",
    "\t\". x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver Callbacks\n",
    "\n",
    "Some optimzation solvers allow you to modify the problem in some way while the solver runs using callbacks.  This is covered in JuMP's documentation [here](http://www.juliaopt.org/JuMP.jl/0.18/callbacks.html).  This isn't supported by all solvers, and those that do tend to be commercial solvers ([see the solver table here](http://www.juliaopt.org/)).\n",
    "\n",
    "We'll demo this using [Gurobi](https://github.com/JuliaOpt/Gurobi.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Gurobi\n",
    "using JuMP, Gurobi\n",
    "m = Model(solver=GurobiSolver())\n",
    "\n",
    "# very large bounds on x,y\n",
    "@variable(m, -10 <= x <= 10, Int)\n",
    "@variable(m, -10 <= y <= 10, Int)\n",
    "\n",
    "@objective(m, Max, x^2+y^2 - y)\n",
    "\n",
    "\n",
    "# L1 ball\n",
    "# |x| + |y| <= 3\n",
    "function l1_ball(cb)\n",
    "    x_val = getvalue(x)\n",
    "    y_val = getvalue(y)\n",
    "    sx = sign(x_val)\n",
    "    sy = sign(y_val)\n",
    "    ax = sx * x_val # |x|\n",
    "    ay = sy * y_val # |y|\n",
    "    \n",
    "    TOL = 1e-6\n",
    "    l1r = 3 # L1 ball radius\n",
    "    \n",
    "    # add lazy constraint if we're outside of L1 ball by at least TOL\n",
    "    if ax + ay > l1r + TOL\n",
    "       @lazyconstraint(cb, sx*x + sy*y <= l1r) \n",
    "    end\n",
    "end # end callback fn\n",
    "\n",
    "addlazycallback(m, l1_ball)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(m)\n",
    "# Print our final solution\n",
    "println(\"Final solution: [ $(getvalue(x)), $(getvalue(y)) ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - NP-complete problems\n",
    "\n",
    "[NP-complete](https://en.wikipedia.org/wiki/NP-completeness) problems are decision problems that may have no polynomial-time algorithm to compute (if $P\\ne NP$), although solutions can be verified in polynomial time.  They arise in computer science, operations research, and have a variety of applications.\n",
    "\n",
    "Many of these problems can be formulated as optimization problems with integer constraints.\n",
    "\n",
    "* Minimum Vertex Cover https://en.wikipedia.org/wiki/Vertex_cover\n",
    "* Maximum Coverage https://en.wikipedia.org/wiki/Maximum_coverage_problem\n",
    "* Max-Cut (Need commercial solver for QIP)\n",
    "\n",
    "While finding an optimal solution may be difficult, in practice we can find good results with IPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum-Coverage\n",
    "\n",
    "The Maximum-Coverage problem is as follows: given $m$ sets, and an integer $k$, maximize the number of elements covered by at most $k$ sets. We'll assume that each of the $m$ sets is a subset of $N$ elements.\n",
    "\n",
    "An application to think of: you want to buy $N$ items.  There are $m$ stores you could possibly go to, and each store only carries some of the items you'd like to buy.  Unfortunately you only have time to go to $k$ of the stores, but you'd like to get as much of your shopping done as possible (maximize the number of items you get).  Which stores should you go to today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a Maximum-Coverage problem\n",
    "# INPUTS:\n",
    "# N = Number of items\n",
    "# m = number of sets\n",
    "# nsamples = number of sets for each sample\n",
    "# OUTPUTS:\n",
    "# S - Vector of Vectors\n",
    "# S[i] = array of sets that contain ith element\n",
    "function gen_max_cover(N, m, nsamples=2)\n",
    "\n",
    "    # create assign elements to the nS sets\n",
    "    S = Vector{Vector{Int64}}(N)\n",
    "    \n",
    "    for i = 1:N\n",
    "       S[i] = unique(sample(1:m, nsamples))\n",
    "    end\n",
    "    return S\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up a max-cover problem\n",
    "k = 2 # number of sets\n",
    "\n",
    "N = 10\n",
    "nS = 5\n",
    "S = gen_max_cover(N, nS)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Choose $k$ sets at random.  How many items are covered?  Try running a few trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Write an integer program using JuMP.  How many items are covered?  Try running a few trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Programming\n",
    "\n",
    "JuMP also allows you to model more general [nonlinear problems (NLPs)](https://en.wikipedia.org/wiki/Nonlinear_programming).  To do this, you will want to use the macros `@NLobjective`, and `@NLconstraint`.  These can be combined with objectives and constraints we've already seen.\n",
    "\n",
    "* JuMP's Introduction to solving NLPs [here](http://www.juliaopt.org/JuMP.jl/0.18/nlp.html)\n",
    "\n",
    "## Example\n",
    "\n",
    "Here, we'll look at the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function)\n",
    "$$\n",
    "f(x,y) = (a-x)^2 + b(y-x^2)^2\n",
    "$$\n",
    "For $b>0$, this function has a global minimum at $(x,y) = (a,a^2)$, where $f(x,y) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plotlyjs()\n",
    "a = 1.0\n",
    "b = 50.0\n",
    "f(x,y) = (a - x)^2 + b*(y - x^2)^2\n",
    "n = 100\n",
    "xs = ones(n)*linspace(-1.5,1.5,n)'\n",
    "ys = linspace(-1.5,1.5,n)*ones(n)'\n",
    "fs = f.(xs, ys)\n",
    "surface(xs, ys, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From http://www.juliaopt.org/JuMP.jl/0.18/nlp.html\n",
    "using JuMP, Ipopt\n",
    "m = Model(solver=IpoptSolver())\n",
    "@variable(m, x, start = 0.0)\n",
    "@variable(m, y, start = 0.0)\n",
    "\n",
    "@NLobjective(m, Min, (1-x)^2 + 100(y-x^2)^2)\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Most optimization solvers need gradients and hessians in order to work.  How does JuMP obtain this information?\n",
    "\n",
    "The answer is that all this information is obtained through [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) which uses the rules of differential calculus to differentiate functions just like you would.  Note that **this is different from using finite difference schemes** and is typically much more accurate.\n",
    "\n",
    "The packages that JuMP uses for automatic differentiation are\n",
    "* [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)\n",
    "* [Calculus.jl](https://github.com/JuliaMath/Calculus.jl)\n",
    "\n",
    "You can also use these packages for your own purposes outside of JuMP.  As long as a function is built from core functions (e.g. you aren't calling special function libraries), products and sums, you can differentiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "f(x::Vector) = sum(x)\n",
    "n = 5\n",
    "x = randn(5)\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.gradient(f,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.hessian(f,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also wrap the gradient and hessian functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x) = ForwardDiff.gradient(f,x)\n",
    "g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example on a more complicated function\n",
    "f(x::Vector) = x[1]*x[2] + sum(sin.(x[3:end]))\n",
    "g(x) = ForwardDiff.gradient(f,x)\n",
    "g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Custom Functions for NLP in JuMP\n",
    "\n",
    "Above, we wrote the Rosenbrock function explicitly as the objective function\n",
    "```julia\n",
    "@NLobjective(m, Min, (1-x)^2 + 100(y-x^2)^2)\n",
    "```\n",
    "We can also provide a function wrapper for the function - all we need to do is \"register\" the function in JuMP.  You can find informaiton in [JuMP's documentation here](http://www.juliaopt.org/JuMP.jl/0.18/nlp.html#user-defined-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt\n",
    "\n",
    "rosenbrock(x,y) = (a - x)^2 + b*(y - x^2)^2\n",
    "\n",
    "m = Model(solver=IpoptSolver(print_level=0))\n",
    "\n",
    "# registers function with JuMP, and derivatives are computed\n",
    "JuMP.register(m, :rosenbrock, 2, rosenbrock, autodiff=true)\n",
    "\n",
    "@variable(m, x, start = 0.0)\n",
    "@variable(m, y, start = 0.0)\n",
    "\n",
    "@NLobjective(m, Min, rosenbrock(x,y))\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the register command:\n",
    "```julia\n",
    "JuMP.register(m, :rosenbrock, 2, rosenbrock, autodiff=true)\n",
    "```\n",
    "| argument | description |\n",
    "| ------ | -------- |\n",
    "| `m` | model |\n",
    "| `:rosenbrock` | Symbol used to identify funciton in objective|\n",
    "| `2` | number of scalar inputs |\n",
    "| `rosenbrock` | function we've declared |\n",
    "\n",
    "If you can't forward diff your package, or you have an optimized gradient, you can also pass those in explicitly.  Note that JuMP currently doesn't support Hessians for functions of more than one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x,y) = x^2 + y^2\n",
    "function ∇f(g,x,y) \n",
    "    g[1] = 2*x\n",
    "    g[2] = 2*y\n",
    "end\n",
    "\n",
    "using JuMP, Ipopt\n",
    "\n",
    "m = Model(solver=IpoptSolver())\n",
    "\n",
    "# registers function with JuMP, and derivatives are computed\n",
    "JuMP.register(m, :f, 2, f, ∇f)\n",
    "\n",
    "@variable(m, x, start = 1.0)\n",
    "@variable(m, y, start = 1.0)\n",
    "\n",
    "@NLobjective(m, Min, f(x,y))\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Constraints\n",
    "\n",
    "You can specify noninear constraints using the `@NLconstraint` macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt\n",
    "\n",
    "m = Model(solver=IpoptSolver())\n",
    "\n",
    "@variable(m, x, start = 1.0)\n",
    "@variable(m, y, start = 1.0)\n",
    "\n",
    "@NLobjective(m, Min, (x-2)^4 + y^2)\n",
    "\n",
    "@NLconstraint(m, x^2 - y == 0)\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Finding the largest eigenvalue of a symmetric matrix can be formulated as an optimization problem\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{maximize} &~x^T A x\\\\\n",
    "\\text{subject to:} &~ \\|x\\|_2^2 = 1\n",
    "\\end{align*}\n",
    "\n",
    "The constraint $\\|x\\|_2^2 = x^Tx = 1$ is non-linear (and non-convex).  Find the largest eigenpair of `A = Diagonal([2; 1])` using JuMP.\n",
    "\n",
    "Try this on a larger (symmetric) matrix.  How long does this take compared to `eig`, or `eigs`?\n",
    "\n",
    "To time an operation, use Julia's `@time` macro, e.g. `@time solve(m)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Find a local optimum for the folowing optimization problem\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{x,y,z}{\\text{minimize}} &~x^3 + y^2 - x^4 + y + z\\\\\n",
    "\\text{subject to:} &~x^2 + y^2 + z^2 \\le 10\\\\\n",
    "&z^3 > 2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "We'll try finding an optimal point on a [lemniscate](https://en.wikipedia.org/wiki/Lemniscate):\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{x,y}{\\text{maximize}} &~ x^3 + y^3\\\\\n",
    "\\text{subject to:} &~ x^4 - x^2 + y^2 = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4\n",
    "\n",
    "There's a short HW 4 posted in the [hw folder](../../hw/hw4/hw4.md).  Feel free to start in-class.\n",
    "\n",
    "# Extras\n",
    "\n",
    "* We used the Ipopt solver in our demo NLPs.  Check out [NLopt.jl](https://github.com/JuliaOpt/NLopt.jl) for another (free/open) option.  This is an itnterface to the [NLopt](https://nlopt.readthedocs.io/en/latest/) library. \n",
    "* Check out [JuMP's examples](http://www.juliaopt.org/notebooks/) for some interesting applied problems, such as rocket trajectories and solving sudoku"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
